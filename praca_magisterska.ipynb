{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugZ_RQon5ZDd"
      },
      "outputs": [],
      "source": [
        "!pip install smote-variants\n",
        "import os\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "\n",
        "from pandas.core.internals.array_manager import new_block\n",
        "from imblearn.datasets import fetch_datasets\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import smote_variants as sv\n",
        "###########################\n",
        "import sys\n",
        "sys.path.append('/content/cWGAN')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from dataloader import load_data\n",
        "from helpers import get_cat_dims\n",
        "\n",
        "from models import WGANGP\n",
        "\n",
        "import logging\n",
        "###########################\n",
        "sys.path.append('/content/CTAB-GAN')\n",
        "from ctabgan import CTABGAN\n",
        "###########################\n",
        "sys.path.append('/content/SMOTified GAN')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision.datasets.utils import download_url\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from GANs_model import GANs_Discriminator, GANs_Generator\n",
        "from model_trainer import train_discriminator, train_generator\n",
        "from model_fit import SG_fit, G_fit\n",
        "from test_model import test_model, test_model_lists\n",
        "from Abalone_data_preprocessing import two_classes_Abalone, four_classes_Abalone, get_features, get_labels, GANs_two_class_real_data, GANs_four_class_real_data\n",
        "from choose_device import get_default_device, to_device, DeviceDataLoader\n",
        "from fit import f1\n",
        "###########################\n",
        "sys.path.append('/content/ConvGeN')\n",
        "from library.dataset import DataSet\n",
        "from library.generators.ConvGeN import ConvGeN\n",
        "###########################\n",
        "sys.path.append('/content/CIGAN')\n",
        "from CIGAN import CIGAN\n",
        "###########################\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "#import os\n",
        "import random\n",
        "import tensorflow\n",
        "\n",
        "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
        "#os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "#!CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.use_deterministic_algorithms(True)\n",
        "tensorflow.random.set_seed(0)\n",
        "\n",
        "ecoli = fetch_datasets()['ecoli']\n",
        "abalone_19 = fetch_datasets()['abalone_19']\n",
        "abalone = fetch_datasets()['abalone']\n",
        "spectrometer = fetch_datasets()['spectrometer']\n",
        "us_crime = fetch_datasets()['us_crime']\n",
        "\n",
        "dataset = us_crime\n",
        "\n",
        "rate = 1.5\n",
        "\n",
        "X = np.array(dataset[\"data\"])\n",
        "y = np.array(dataset[\"target\"])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "#df = pd.DataFrame(dataset.data)\n",
        "#dft = pd.DataFrame(dataset.target)\n",
        "#dfc = pd.concat([df, dft], axis=1)\n",
        "#dfc.columns = [*dfc.columns[:-1], df.shape[1]]\n",
        "#dfc.to_csv('/content/dataset.csv', index=False)\n",
        "\n",
        "u, c = np.unique(y_train, return_counts = True)\n",
        "minority_class = u[0]\n",
        "majority_class = u[1]\n",
        "minority_class_count = c[0]\n",
        "majority_class_count = c[1]\n",
        "if c[1] < c[0]:\n",
        "  minority_class = u[1]\n",
        "  majority_class = u[0]\n",
        "  minority_class_count = c[1]\n",
        "  majority_class_count = c[0]\n",
        "  \n",
        "def test_SMOTE(X_train, y_train):\n",
        "  X_resampled, y_resampled = sv.SMOTE(random_state=0, nn_params={'metric': 'euclidean'}, proportion=rate).sample(X_train, y_train)\n",
        "  return X_resampled, y_resampled\n",
        "def test_Borderline_SMOTE1(X_train, y_train):\n",
        "  X_resampled, y_resampled = sv.Borderline_SMOTE1(random_state=0, nn_params={'metric': 'euclidean'}, proportion=rate).sample(X_train, y_train)\n",
        "  return X_resampled, y_resampled\n",
        "def test_Borderline_SMOTE2(X_train, y_train):\n",
        "  X_resampled, y_resampled = sv.Borderline_SMOTE2(random_state=0, nn_params={'metric': 'euclidean'}, proportion=rate).sample(X_train, y_train)\n",
        "  return X_resampled, y_resampled\n",
        "def test_ADASYN(X_train, y_train):\n",
        "  X_resampled, y_resampled = sv.ADASYN(random_state=0, nn_params={'metric': 'euclidean'}, proportion=rate).sample(X_train, y_train)\n",
        "  return X_resampled, y_resampled\n",
        "def test_AND_SMOTE(X_train, y_train):\n",
        "  X_resampled, y_resampled = sv.AND_SMOTE(random_state=0, nn_params={'metric': 'euclidean'}, proportion=rate).sample(X_train, y_train)\n",
        "  return X_resampled, y_resampled\n",
        "def test_SOI_CJ(X_train, y_train):\n",
        "  X_resampled, y_resampled = sv.SOI_CJ(random_state=0, nn_params={'metric': 'euclidean'}, proportion=rate).sample(X_train, y_train)\n",
        "  return X_resampled, y_resampled\n",
        "\n",
        "X_resampled_a, y_resampled_a = test_SMOTE(X_train, y_train)\n",
        "X_resampled_b1, y_resampled_b1 = test_Borderline_SMOTE1(X_train, y_train)\n",
        "X_resampled_b2, y_resampled_b2 = test_Borderline_SMOTE2(X_train, y_train)\n",
        "X_resampled_c, y_resampled_c = test_ADASYN(X_train, y_train)\n",
        "X_resampled_d, y_resampled_d = test_AND_SMOTE(X_train, y_train)\n",
        "X_resampled_e, y_resampled_e = test_SOI_CJ(X_train, y_train)\n",
        "\n",
        "#X_resampled_a, y_resampled_a = sv.SMOTE(random_state=0).sample(X_train, y_train)\n",
        "#X_resampled_b1, y_resampled_b1 = sv.Borderline_SMOTE1(random_state=0).sample(X_train, y_train)\n",
        "#X_resampled_b2, y_resampled_b2 = sv.Borderline_SMOTE2(random_state=0).sample(X_train, y_train)\n",
        "#X_resampled_c, y_resampled_c = sv.ADASYN(random_state=0).sample(X_train, y_train)\n",
        "#X_resampled_d, y_resampled_d = sv.AND_SMOTE(random_state=0).sample(X_train, y_train)\n",
        "#X_resampled_e, y_resampled_e = sv.SOI_CJ(random_state=0).sample(X_train, y_train)\n",
        "##################################################cWGAN\n",
        "def prep_for_cWGAN():\n",
        "  # preprocess data\n",
        "  num_prep = make_pipeline(SimpleImputer(strategy='mean'),\n",
        "                          MinMaxScaler())\n",
        "  cat_prep = make_pipeline(SimpleImputer(strategy='most_frequent'),\n",
        "                          OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "  num_cols = [i for i in range(X_train.shape[1])]\n",
        "  cat_cols = []\n",
        "  prep = ColumnTransformer([\n",
        "      ('num', num_prep, num_cols),\n",
        "      ('cat', cat_prep, cat_cols)],\n",
        "      remainder='drop')\n",
        "  return prep\n",
        "def test_cWGANGP(X_train, y_train):\n",
        "  #to make no_oversampling working properly\n",
        "  u, c = np.unique(y_train, return_counts = True)\n",
        "  minority_class = u[0]\n",
        "  majority_class = u[1]\n",
        "  minority_class_count = c[0]\n",
        "  majority_class_count = c[1]\n",
        "  if c[1] < c[0]:\n",
        "    minority_class = u[1]\n",
        "    majority_class = u[0]\n",
        "    minority_class_count = c[1]\n",
        "    majority_class_count = c[0]\n",
        "\n",
        "  num_cols = [i for i in range(X_train.shape[1])]\n",
        "  cat_cols = []\n",
        "  prep = prep_for_cWGAN()\n",
        "  X_train_trans = prep.fit_transform(X_train)\n",
        "  y_train_trans = np.where(y_train == 1, 1, 0)\n",
        "  gan = WGANGP(write_to_disk=True, # whether to create an output folder. Plotting will be surpressed if flase\n",
        "              compute_metrics_every=1250, print_every=2500, plot_every=1000000,\n",
        "              num_cols = num_cols, cat_dims=[],\n",
        "              # pass the one hot encoder to the GAN to enable count plots of categorical variables\n",
        "              transformer=prep.named_transformers_['cat']['onehotencoder'],\n",
        "              # pass column names to enable\n",
        "              cat_cols=cat_cols,\n",
        "              use_aux_classifier_loss=True,\n",
        "              d_updates_per_g=3, gp_weight=15)\n",
        "  gan.fit(X_train_trans, y=y_train_trans, \n",
        "          condition=True,\n",
        "          epochs=300,  \n",
        "          batch_size=64,\n",
        "          netG_kwargs = {'hidden_layer_sizes': (128,64), \n",
        "                          'n_cross_layers': 1,\n",
        "                          'cat_activation': 'gumbel_softmax',\n",
        "                          'num_activation': 'none',\n",
        "                          'condition_num_on_cat': False,#True, \n",
        "                          'noise_dim': 30, \n",
        "                          'normal_noise': False,\n",
        "                          'activation':  'leaky_relu',\n",
        "                          'reduce_cat_dim': True,\n",
        "                          'use_num_hidden_layer': True,\n",
        "                          'layer_norm':False,},\n",
        "          netD_kwargs = {'hidden_layer_sizes': (128,64,32),\n",
        "                          'n_cross_layers': 2,\n",
        "                          'embedding_dims': 'auto',\n",
        "                          'activation':  'leaky_relu',\n",
        "                          'sigmoid_activation': False,\n",
        "                          'noisy_num_cols': True,\n",
        "                          'layer_norm':True,}\n",
        "        )\n",
        "  X_resampled_1, y_resampled_1 = gan.resample(X_train_trans, y_train_trans, int((majority_class_count - minority_class_count)*rate))\n",
        "  y_resampled_1 = np.where(y_resampled_1 == 1, 1, -1)\n",
        "  return X_resampled_1, y_resampled_1\n",
        "X_resampled_1, y_resampled_1 = test_cWGANGP(X_train, y_train)\n",
        "##################################################\n",
        "##################################################CTAB-GAN\n",
        "no_oversampling_test_split = False\n",
        "def test_CTABGAN(X_train, y_train):\n",
        "  df = pd.DataFrame(X_train)\n",
        "  dft = pd.DataFrame(y_train)\n",
        "  dfc = pd.concat([df, dft], axis=1)\n",
        "  dfc.columns = [*dfc.columns[:-1], df.shape[1]]\n",
        "  dfc.to_csv('/content/dataset.csv', index=False)\n",
        "\n",
        "  num_cols = [i for i in range(X_train.shape[1])]\n",
        "  synthesizer =  CTABGAN(raw_csv_path = '/content/dataset.csv',\n",
        "                  test_ratio = 0,  \n",
        "                  categorical_columns = [], \n",
        "                  log_columns = [],\n",
        "                  mixed_columns= {}, \n",
        "                  integer_columns = num_cols,\n",
        "                  problem_type= {\"Classification\": str(df.shape[1])},\n",
        "                  epochs = 150) \n",
        "\n",
        "  #to make no_oversampling working properly\n",
        "  u, c = np.unique(y_train, return_counts = True)\n",
        "  minority_class = u[0]\n",
        "  majority_class = u[1]\n",
        "  minority_class_count = c[0]\n",
        "  majority_class_count = c[1]\n",
        "  if c[1] < c[0]:\n",
        "    minority_class = u[1]\n",
        "    majority_class = u[0]\n",
        "    minority_class_count = c[1]\n",
        "    majority_class_count = c[0]\n",
        "  # Fitting the synthesizer to the training dataset and generating synthetic data\n",
        "  X_resampled_2 = X_train\n",
        "  new_minority_class_count = minority_class_count\n",
        "  ultimate_minority_class_count = int((majority_class_count - minority_class_count)*rate + minority_class_count)\n",
        "  synthesizer.fit()\n",
        "  while (new_minority_class_count<ultimate_minority_class_count):\n",
        "      syn = synthesizer.generate_samples()\n",
        "      syn[str(df.shape[1])] = syn[str(df.shape[1])].round()\n",
        "      syn[str(df.shape[1])] = syn[str(df.shape[1])].apply(lambda x: int(x) if(x == x) else None)\n",
        "      new_minority = syn[syn[str(df.shape[1])]==minority_class]\n",
        "      new_minority = new_minority.drop(columns=[str(df.shape[1])])\n",
        "      new_minority_class_count += new_minority.shape[0]\n",
        "      if (new_minority_class_count > ultimate_minority_class_count):\n",
        "        diff = new_minority_class_count-ultimate_minority_class_count\n",
        "        new_minority = new_minority.drop(new_minority.tail(diff).index)\n",
        "        X_resampled_2 = np.vstack((X_resampled_2, new_minority.to_numpy()))\n",
        "        break\n",
        "      X_resampled_2 = np.vstack((X_resampled_2, new_minority.to_numpy()))\n",
        "  y_resampled_2 = np.concatenate([y_train, np.full((X_resampled_2.shape[0]-X_train.shape[0]), minority_class)])\n",
        "  return X_resampled_2, y_resampled_2\n",
        "X_resampled_2, y_resampled_2 = test_CTABGAN(X_train, y_train)\n",
        "##################################################\n",
        "##################################################SMOTified-GAN\n",
        "def test_SMOTified_GAN(X_train, y_train):\n",
        "  def shuffle_in_unison(a, b):     #Shuffling the features and labels in unison.\n",
        "      assert len(a) == len(b)       #In Python, the assert statement is used to continue the execute if the given condition evaluates to True.\n",
        "      shuffled_a = np.empty(a.shape, dtype=a.dtype)       #Return a new array of given shape and type, without initializing entries.\n",
        "      shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
        "      permutation = np.random.permutation(len(a))\n",
        "      for old_index, new_index in enumerate(permutation):\n",
        "          shuffled_a[new_index] = a[old_index]\n",
        "          shuffled_b[new_index] = b[old_index]\n",
        "      return shuffled_a, shuffled_b\n",
        "\n",
        "  def GANs_real_data(X_train, y_train, minority_class):   #Defining the real data for GANs\n",
        "    X_real = []\n",
        "    y_train = y_train.ravel()\n",
        "    for i in range(len(y_train)):\n",
        "      if int(y_train[i])==minority_class:\n",
        "        X_real.append(X_train[i])\n",
        "    X_real = np.array(X_real)\n",
        "    y_real = np.full((X_real.shape[0],), minority_class)\n",
        "    return X_real, y_real\n",
        "\n",
        "  #to make no_oversampling working properly\n",
        "  u, c = np.unique(y_train, return_counts = True)\n",
        "  minority_class = u[0]\n",
        "  majority_class = u[1]\n",
        "  minority_class_count = c[0]\n",
        "  majority_class_count = c[1]\n",
        "  if c[1] < c[0]:\n",
        "    minority_class = u[1]\n",
        "    majority_class = u[0]\n",
        "    minority_class_count = c[1]\n",
        "    majority_class_count = c[0]\n",
        "\n",
        "  #special conditions for rates: 0.5, 1.0, 1.5\n",
        "  if rate==0.5:\n",
        "    X_train_SMOTE,y_train_SMOTE = SMOTE(sampling_strategy=rate+(minority_class_count/majority_class_count)*rate, random_state=0).fit_resample(X_train,y_train)\n",
        "  elif rate==1.0:\n",
        "    X_train_SMOTE,y_train_SMOTE = SMOTE(sampling_strategy=rate, random_state=0).fit_resample(X_train,y_train)\n",
        "  elif rate==1.5:\n",
        "    X_train_SMOTE_a,y_train_SMOTE_a = SMOTE(sampling_strategy=1.0, random_state=0).fit_resample(X_train,y_train)\n",
        "    X_train_SMOTE_b,y_train_SMOTE_b = SMOTE(sampling_strategy=0.5-(minority_class_count/majority_class_count)*0.5, random_state=0).fit_resample(X_train,y_train)\n",
        "    X_train_SMOTE_b = X_train_SMOTE_b[y_train_SMOTE_b[:]==minority_class]\n",
        "    y_train_SMOTE_b = y_train_SMOTE_b[y_train_SMOTE_b[:]==minority_class]\n",
        "    X_train_SMOTE = np.concatenate((X_train_SMOTE_a, X_train_SMOTE_b), axis=0)\n",
        "    y_train_SMOTE = np.concatenate((y_train_SMOTE_a, y_train_SMOTE_b), axis=0)\n",
        "  else:\n",
        "    X_train_SMOTE,y_train_SMOTE = SMOTE(sampling_strategy=rate, random_state=0).fit_resample(X_train,y_train)\n",
        "  device = get_default_device()\n",
        "\n",
        "  ##### Oversampled data from SMOTE that is now to be passed in SMOTified GANs #####\n",
        "  X_oversampled = X_train_SMOTE[(X_train.shape[0]):]\n",
        "  X_oversampled = torch.from_numpy(X_oversampled)\n",
        "  X_oversampled = to_device(X_oversampled.float(), device)\n",
        "\n",
        "  lr = 0.0002\n",
        "  epochs = 150\n",
        "  batch_size = 128\n",
        "\n",
        "  X_real, y_real = GANs_real_data(X_train, y_train, minority_class)   #Defining the real data to be put in GANs\n",
        "\n",
        "  #Training our SMOTified GANs and GANs model and fetching their trained generators.\n",
        "  generator_SG, generator_G = f1(X_train, y_train, X_train_SMOTE, y_train_SMOTE, X_real, y_real, X_oversampled, device, lr, epochs, batch_size, minority_class, majority_class)\n",
        "\n",
        "  Trained_X_oversampled_SG = generator_SG(X_oversampled.float().to(device)).cpu().detach().numpy()\n",
        "  Trained_SG_dataset = np.concatenate((X_train_SMOTE[:(X_train.shape[0])], Trained_X_oversampled_SG), axis=0)\n",
        "  X_resampled_3, y_resampled_3 = shuffle_in_unison(Trained_SG_dataset, y_train_SMOTE)\n",
        "  return X_resampled_3, y_resampled_3\n",
        "X_resampled_3, y_resampled_3 = test_SMOTified_GAN(X_train, y_train)\n",
        "##################################################\n",
        "##################################################ConvGeN\n",
        "def test_ConvGeN(X_train, y_train):\n",
        "  def loadData(X_train, y_train):\n",
        "    features = X_train\n",
        "    labels = y_train\n",
        "    label_1 = list(np.where(labels == 1)[0])\n",
        "    label_0 = list(np.where(labels != 1)[0])\n",
        "    features_1 = features[label_1]\n",
        "    features_0 = features[label_0]\n",
        "    return DataSet(data0=features_0, data1=features_1)\n",
        "\n",
        "  #to make no_oversampling working properly\n",
        "  u, c = np.unique(y_train, return_counts = True)\n",
        "  minority_class = u[0]\n",
        "  majority_class = u[1]\n",
        "  minority_class_count = c[0]\n",
        "  majority_class_count = c[1]\n",
        "  if c[1] < c[0]:\n",
        "    minority_class = u[1]\n",
        "    majority_class = u[0]\n",
        "    minority_class_count = c[1]\n",
        "    majority_class_count = c[0]\n",
        "\n",
        "  data = loadData(X_train, y_train)\n",
        "  gen = ConvGeN(data.data0.shape[1], neb=5)#, neb_epochs=3)\n",
        "  gen.reset(data)\n",
        "  gen.train(data)\n",
        "  syntheticPoints = gen.generateData(int((majority_class_count - minority_class_count)*rate))\n",
        "\n",
        "  X_resampled_4 = np.vstack((X_train, syntheticPoints))\n",
        "  y_resampled_4 = np.concatenate([y_train, np.full((X_resampled_4.shape[0]-X_train.shape[0]), minority_class)])\n",
        "  return X_resampled_4, y_resampled_4\n",
        "X_resampled_4, y_resampled_4 = test_ConvGeN(X_train, y_train)\n",
        "##################################################\n",
        "##################################################CIGAN\n",
        "def test_CIGAN(X_train, y_train):\n",
        "  #to make no_oversampling working properly\n",
        "  u, c = np.unique(y_train, return_counts = True)\n",
        "  minority_class = u[0]\n",
        "  majority_class = u[1]\n",
        "  minority_class_count = c[0]\n",
        "  majority_class_count = c[1]\n",
        "  if c[1] < c[0]:\n",
        "    minority_class = u[1]\n",
        "    majority_class = u[0]\n",
        "    minority_class_count = c[1]\n",
        "    majority_class_count = c[0]\n",
        "\n",
        "  # The CIGAN\n",
        "  cigan = CIGAN(X_train, y_train, int((majority_class_count - minority_class_count)*rate), generator_learning_rate=10 ** -4, discriminator_learning_rate=10 ** -4)\n",
        "\n",
        "  # Augment the minority classes in the training data\n",
        "  X_resampled_5, y_resampled_5 = cigan.fit_resample(X_train, y_train)\n",
        "  return X_resampled_5, y_resampled_5\n",
        "X_resampled_5, y_resampled_5 = test_CIGAN(X_train, y_train)\n",
        "##################################################\n",
        "def check_no_oversampling(X_train, y_train, minority_class, majority_class):\n",
        "  def HasD(x, y):\n",
        "    total = 0\n",
        "    for xi, yi in zip(x, y):\n",
        "        min_value = min(xi, yi)\n",
        "        max_value = max(xi, yi)\n",
        "        total += 1 # we sum the 1 in both cases\n",
        "        if min_value >= 0:\n",
        "            total -= (1 + min_value) / (1 + max_value)\n",
        "        else:\n",
        "            # min_value + abs(min_value) = 0, so we ignore that\n",
        "            total -= 1 / (1 + max_value + abs(min_value))\n",
        "    return total\n",
        "\n",
        "  minoridx=np.where(y_train==minority_class)[0]\n",
        "  majoridx=np.where(y_train==majority_class)[0]\n",
        "  Xminor=X_train[minoridx,:]\n",
        "  Xmajor=X_train[majoridx,:]\n",
        "  yminor=y_train[minoridx]\n",
        "  ymajor=y_train[majoridx]\n",
        "\n",
        "  print(f'The data has {Xminor.shape[0]}  minority samples, labeled {(minority_class)}, and {Xmajor.shape[0]} majority samples, labeled {(majority_class)}')\n",
        "  print(Xminor.shape[1])\n",
        "\n",
        "  minoritypercent=0.0\n",
        "  majoritypercent=0.50\n",
        "\n",
        "  Nminortoremove=np.fix(len(Xminor)*minoritypercent).astype(np.int64)\n",
        "  Nmajortoremove=np.fix(len(Xmajor)*majoritypercent).astype(np.int64)\n",
        "\n",
        "  print(f'{Nminortoremove} from minority and {Nmajortoremove} from majority will be removed')\n",
        "\n",
        "  Xhiddenmajor=Xmajor[0:Nmajortoremove,:]\n",
        "  yhiddenmajor=ymajor[0:Nmajortoremove]\n",
        "\n",
        "  Xnewminor=Xminor[Nminortoremove:]\n",
        "  Xnewmajor=Xmajor[Nmajortoremove:]\n",
        "  ynewminor=yminor[Nminortoremove:]\n",
        "  ynewmajor=ymajor[Nmajortoremove:]  \n",
        "\n",
        "  print(f'the number of hidden examples is {Xhiddenmajor.shape[0]}')\n",
        "\n",
        "  XnewData=np.vstack((Xnewmajor,Xnewminor))\n",
        "  ynewData=np.hstack((ynewmajor,ynewminor))\n",
        "  sizeofnewdata=XnewData.shape[0]\n",
        "\n",
        "  # stack the hidden data with the original to find the similarity \n",
        "  XKNN=np.vstack((XnewData,Xhiddenmajor))\n",
        "  yKNN=np.hstack((ynewData,yhiddenmajor))\n",
        "\n",
        "  print(f'The new data after removing samples has {sizeofnewdata} samples, {Xnewminor.shape[0]} minority and {Xnewmajor.shape[0]} majority ')\n",
        "\n",
        "  SuccNames=[]\n",
        "  ErrorPercentages=[]\n",
        "  ErrorCounts=[]\n",
        "  methods = [test_SMOTE, test_Borderline_SMOTE1, test_Borderline_SMOTE2, test_ADASYN,\n",
        "             test_AND_SMOTE, test_SOI_CJ,test_cWGANGP, test_CTABGAN,\n",
        "             test_SMOTified_GAN, test_ConvGeN, test_CIGAN]\n",
        "  string_a = []\n",
        "  string_b = []\n",
        "  for methodidx in methods:\n",
        "      try:        \n",
        "        X_samp, y_samp= methodidx(XnewData, ynewData)\n",
        "        resambledX=X_samp[sizeofnewdata:]\n",
        "        resambledy=y_samp[sizeofnewdata:]\n",
        "        #string_a.append(f'The number of added samples is {resambledX.shape[0]}')\n",
        "        #print(f'The number of added samples is {resambledX.shape[0]}')\n",
        "        neigh = KNeighborsClassifier(n_neighbors=1, metric=HasD)\n",
        "        neigh.fit(XKNN, yKNN)\n",
        "\n",
        "        predis=neigh.predict(resambledX)\n",
        "        predis==majority_class\n",
        "        countwrong=np.count_nonzero(predis==majority_class)\n",
        "        Errorpercent= countwrong/len(predis)\n",
        "        string_a.append(f'The number of added samples is {resambledX.shape[0]}, X_samp is {X_samp.shape[0]} and sizeofnewdata is {sizeofnewdata}')\n",
        "        string_b.append(f'The error percent of {methodidx.__name__} is {Errorpercent}, with number of wrong predictions = {countwrong} out of {resambledX.shape[0]}')\n",
        "        #print(f'The error percent of {methodidx.__name__} is {Errorpercent}, with number of wrong predictions = {countwrong} out of {resambledX.shape[0]}')\n",
        "        ErrorPercentages.append(Errorpercent)\n",
        "        ErrorCounts.append(countwrong)\n",
        "        SuccNames.append(methodidx.__name__)\n",
        "\n",
        "      except:\n",
        "        print(\"EXCEPT\")\n",
        "        #print(f'cannot perform using {MethodName}')\n",
        "        #writer.writerow([MethodName,'-','-','-','-','-','-','-','-'])\n",
        "  for a, b in zip(string_a, string_b):\n",
        "    print(a)\n",
        "    print(b)\n",
        "##################################################\n",
        "no_oversampling_test_split = True\n",
        "check_no_oversampling(X_train, y_train, minority_class, majority_class)\n",
        "\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "fig, ax =plt.subplots(4,3, figsize=(25,25))\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "\n",
        "def shape(X, y):\n",
        "  features = X\n",
        "  labels = y\n",
        "  label_1 = list(np.where(labels == 1)[0])\n",
        "  label_0 = list(np.where(labels != 1)[0])\n",
        "  features_1 = features[label_1]\n",
        "  features_0 = features[label_0]\n",
        "  print(features_1.shape)\n",
        "  print(features_0.shape)\n",
        "\n",
        "def results(X_resampled, y_resampled, X_test, y_test, cls, name):\n",
        "  cls.fit(X_resampled, y_resampled)\n",
        "  y_pred = cls.predict(X_test)\n",
        "  print(name)\n",
        "  print(classification_report_imbalanced(y_test, y_pred))\n",
        "  #print(precision_recall_fscore_support(y_test, y_pred))\n",
        "  print('f1_score')\n",
        "  print(f1_score(y_test, y_pred))\n",
        "  print('geometric_mean_score')\n",
        "  print(geometric_mean_score(y_test, y_pred))\n",
        "  print('accuracy_score')\n",
        "  print(accuracy_score(y_test, y_pred))\n",
        "  print('balanced_accuracy_score')\n",
        "  print(balanced_accuracy_score(y_test, y_pred))\n",
        "  RocCurveDisplay.from_estimator(cls, X_test, y_test, name = cls.__class__)\n",
        "  #RocCurveDisplay.from_predictions(y_test, y_pred, name = cls.__class__)\n",
        "\n",
        "sns_i = 0\n",
        "def first_results(X_resampled, y_resampled, X_test, y_test, cls, name):\n",
        "  global sns_i\n",
        "  z = tsne.fit_transform(X_resampled)\n",
        "  sns.scatterplot(data=z, x=z[:,0], y=z[:,1], hue=y_resampled, palette=sns.color_palette(\"hls\", 3), ax=ax[int(sns_i/3)][sns_i%3]).set(title=name, xlabel=\"Feature 1\", ylabel=\"Feature 2\")\n",
        "  shape(X_resampled, y_resampled)\n",
        "  sns_i = sns_i + 1\n",
        "  results(X_resampled, y_resampled, X_test, y_test, cls, name)\n",
        "\n",
        "cWGAN_prep = prep_for_cWGAN()\n",
        "cWGAN_prep.fit_transform(X_train)\n",
        "cls = KNeighborsClassifier()\n",
        "first_results(X_train, y_train, X_test, y_test, cls, 'No oversampling')\n",
        "first_results(X_resampled_a, y_resampled_a, X_test, y_test, cls, 'SMOTE')\n",
        "first_results(X_resampled_b1, y_resampled_b1, X_test, y_test, cls, 'Borderline-SMOTE1')\n",
        "first_results(X_resampled_b2, y_resampled_b2, X_test, y_test, cls, 'Borderline-SMOTE2')\n",
        "first_results(X_resampled_c, y_resampled_c, X_test, y_test, cls, 'ADASYN')\n",
        "first_results(X_resampled_d, y_resampled_d, X_test, y_test, cls, 'AND SMOTE')\n",
        "first_results(X_resampled_e, y_resampled_e, X_test, y_test, cls, 'SOI-CJ')\n",
        "first_results(X_resampled_1, y_resampled_1, cWGAN_prep.transform(X_test), y_test, cls, 'cWGAN')\n",
        "first_results(X_resampled_2, y_resampled_2, X_test, y_test, cls, 'CTAB-GAN')\n",
        "first_results(X_resampled_3, y_resampled_3, X_test, y_test, cls, 'SMOTified GAN')\n",
        "first_results(X_resampled_4, y_resampled_4, X_test, y_test, cls, 'ConvGeN')\n",
        "first_results(X_resampled_5, y_resampled_5, X_test, y_test, cls, 'CIGAN')\n",
        "\n",
        "cls = SVC(random_state=0)\n",
        "results(X_train, y_train, X_test, y_test, cls, 'No oversampling')\n",
        "results(X_resampled_a, y_resampled_a, X_test, y_test, cls, 'SMOTE')\n",
        "results(X_resampled_b1, y_resampled_b1, X_test, y_test, cls, 'Borderline-SMOTE1')\n",
        "results(X_resampled_b2, y_resampled_b2, X_test, y_test, cls, 'Borderline-SMOTE2')\n",
        "results(X_resampled_c, y_resampled_c, X_test, y_test, cls, 'ADASYN')\n",
        "results(X_resampled_d, y_resampled_d, X_test, y_test, cls, 'AND SMOTE')\n",
        "results(X_resampled_e, y_resampled_e, X_test, y_test, cls, 'SOI-CJ')\n",
        "results(X_resampled_1, y_resampled_1, cWGAN_prep.transform(X_test), y_test, cls, 'cWGAN')\n",
        "results(X_resampled_2, y_resampled_2, X_test, y_test, cls, 'CTAB-GAN')\n",
        "results(X_resampled_3, y_resampled_3, X_test, y_test, cls, 'SMOTified GAN')\n",
        "results(X_resampled_4, y_resampled_4, X_test, y_test, cls, 'ConvGeN')\n",
        "results(X_resampled_5, y_resampled_5, X_test, y_test, cls, 'CIGAN')\n",
        "\n",
        "cls = GaussianNB()\n",
        "results(X_train, y_train, X_test, y_test, cls, 'No oversampling')\n",
        "results(X_resampled_a, y_resampled_a, X_test, y_test, cls, 'SMOTE')\n",
        "results(X_resampled_b1, y_resampled_b1, X_test, y_test, cls, 'Borderline-SMOTE1')\n",
        "results(X_resampled_b2, y_resampled_b2, X_test, y_test, cls, 'Borderline-SMOTE2')\n",
        "results(X_resampled_c, y_resampled_c, X_test, y_test, cls, 'ADASYN')\n",
        "results(X_resampled_d, y_resampled_d, X_test, y_test, cls, 'AND SMOTE')\n",
        "results(X_resampled_e, y_resampled_e, X_test, y_test, cls, 'SOI-CJ')\n",
        "results(X_resampled_1, y_resampled_1, cWGAN_prep.transform(X_test), y_test, cls, 'cWGAN')\n",
        "results(X_resampled_2, y_resampled_2, X_test, y_test, cls, 'CTAB-GAN')\n",
        "results(X_resampled_3, y_resampled_3, X_test, y_test, cls, 'SMOTified GAN')\n",
        "results(X_resampled_4, y_resampled_4, X_test, y_test, cls, 'ConvGeN')\n",
        "results(X_resampled_5, y_resampled_5, X_test, y_test, cls, 'CIGAN')\n",
        "\n",
        "cls = DecisionTreeClassifier(random_state=0)\n",
        "results(X_train, y_train, X_test, y_test, cls, 'No oversampling')\n",
        "results(X_resampled_a, y_resampled_a, X_test, y_test, cls, 'SMOTE')\n",
        "results(X_resampled_b1, y_resampled_b1, X_test, y_test, cls, 'Borderline-SMOTE1')\n",
        "results(X_resampled_b2, y_resampled_b2, X_test, y_test, cls, 'Borderline-SMOTE2')\n",
        "results(X_resampled_c, y_resampled_c, X_test, y_test, cls, 'ADASYN')\n",
        "results(X_resampled_d, y_resampled_d, X_test, y_test, cls, 'AND SMOTE')\n",
        "results(X_resampled_e, y_resampled_e, X_test, y_test, cls, 'SOI-CJ')\n",
        "results(X_resampled_1, y_resampled_1, cWGAN_prep.transform(X_test), y_test, cls, 'cWGAN')\n",
        "results(X_resampled_2, y_resampled_2, X_test, y_test, cls, 'CTAB-GAN')\n",
        "results(X_resampled_3, y_resampled_3, X_test, y_test, cls, 'SMOTified GAN')\n",
        "results(X_resampled_4, y_resampled_4, X_test, y_test, cls, 'ConvGeN')\n",
        "results(X_resampled_5, y_resampled_5, X_test, y_test, cls, 'CIGAN')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "neigh = KNeighborsClassifier()\n",
        "neigh.fit(X_resampled, y_resampled)\n",
        "y_pred = neigh.predict(X_test)\n",
        "\n",
        "svc = SVC(random_state=0)\n",
        "svc.fit(X_resampled, y_resampled)\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_resampled, y_resampled)\n",
        "y_pred = gnb.predict(X_test)#X_test_trans\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}